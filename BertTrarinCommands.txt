python3 create_pretraining_data.py \ 	
	--input_file=gs://tmh_pretrain/WikiText-2.txt --output_file=gs://tmh_pretrain/output/tf.wikitext-2.tfrecord --vocab_file=gs://tmh_pretrain/cased_L-12_H-768_A-12/vocab.txt --random_seed=42 --do_whole_word_mask=True --max_seq_length=128 --max_predictions_per_seq=20 --masked_lm_prob=0.15 --dupe_factor=20
Wrote 610168 total instances


nohup bash -c 'time python3 run_pretraining.py --input_file=gs://tmh_pretrain/o
utput/tf.wikitext-2.tfrecord --output_dir=gs://tmh_pretrain/output/train-out/ --do_train=True --do_eval=True --bert_config_file=gs://tmh_pretrain/cased_L-12_H-768_A-12/bert_config.json --train_batch_size=32 --max_seq_length=128 --max_pr
edictions_per_seq=20 --num_train_steps=10000 --num_warmup_steps=10 --learning_rate=2e-5 --use_tpu=True --tpu_name=bert-train'  &


python3 run_classifier.py   --task_name=MRPC   --do_train=true   --do_eval=true   --data_dir=/home/h_tayyarmadabushi_1/glue_data/MRPC   --vocab_file=gs://tmh_pretrain/cased_L-12_H-768_A-12/vocab.txt --bert_config_file=gs://tmh_pretrain/cased_L-12_H-768_A-12/bert_config.json --init_checkpoint=gs://tmh_pretrain/output/train-out/model.ckpt-20000 --max_seq_length=128 --train_batch_size=32   --learning_rate=2e-5   --num_train_epochs=3.0   --output_dir=gs://tmh_pretrain/outpmrpc_output --use_tpu=True --tpu_name=bert-train

ctpu up --name=bert-train -zone=europe-west4-a --tpu-size=v3-8

ctpu rm --name=bert-train -zone=europe-west4-a